{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"styles/iacs.png\"> S-109A Introduction to Data Science \n",
    "\n",
    "## Homework 1\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Kevin Rader\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Main Theme: Data Collection - Web Scraping - Data Parsing \n",
    "#### Learning Objectives \n",
    "\n",
    "In this homework, your goal is to learn how to acquire, parse, clean, and analyze data. Initially you read the data from a file, then you scrape them directly from a website. You look for specific pieces of information by parsing the data, you clean the data to prepare them for analysis, and finally, you answer some questions.\n",
    "\n",
    "#### Instructions\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- The deliverables in Canvas are: a) This python notebook with your code and answers, b) a .pdf version of this notebook, c) The BibTex file you created.\n",
    "d) The JSON file you created.\n",
    "- Exercise **responsible scraping**. Web servers can become slow or unresponsive if they receive too many requests from the same source in a short amount of time. Use a delay of 10 seconds between requests in your code. This helps not to get blocked by the target website. Run the webpage fetching part of the homework only once and do not re-run after you have saved the results in the JSON file (details below). \n",
    "- Web scraping requests can take several minutes. This is another reason why you should not wait until the last minute to do this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part A [50 pts]:  Help a professor convert his publications to bibTex\n",
    "\n",
    "### Overview\n",
    "\n",
    "In Part 1 your goal is to parse the HTML page of a Professor containing some of his publications, and answer some questions. This page is provided to you in the file `data/publist_super_clean.html`. There are 44 publications in descending order from No. 244 to No. 200.\n",
    "\n",
    "You are to use python's **regular expressions**, a powerful way of parsing text. You may **not** use any parsing tool such as Beautiful Soup yet. In doing so you will get more familiar with three of the common file formats for storing and transferring data, which are:\n",
    "- CSV, a text-based file format used for storing tabular data that are separated by some delimiter, usually comma or space.\n",
    "- HTML/XML, the stuff the web is made of.\n",
    "- JavaScript Object Notation(JSON), a text-based open standard designed for transmitting structured data over the web.\n",
    "\n",
    "### <p class='q1'> Question 1: Parsing using Regular Expressions \n",
    " **1.1** Write a function called `get_pubs` that takes a .html filename as an input and returns a string containing the HTML page in this file (see definition below). Call this function using `data/publist_super_clean.html` as input and name the returned string `prof_pubs`. \n",
    " \n",
    " **1.2** Calculate how many times the author named '`C.M. Friend`' appears in the list of publications. \n",
    " \n",
    " **1.3** Find all unique journals and copy them in a variable named `journals`.  \n",
    " \n",
    " **1.4** Create a list named `pub_authors` whose elements are strings containing the authors' names for each paper. \n",
    "    \n",
    "### Hints\n",
    "- Look for patterns in the HTML tags that reveal where each piece of information such as the title of the paper, the names of the authors, the journal name, is stored. For example, you might notice that the journal name(s) is contained between the &lt;I&gt; HTML tag.\n",
    "- Each publication has multiple authors. \n",
    "- `C.M. Friend` also shows up as `Cynthia M. Friend` in the file.  Count just `C. M. Friend`. \n",
    "- There is a comma at the end of the string of authors. You can choose to keep it in the string or remove it and put it back when you write the string as a BibTex entry. \n",
    "- You want to remove duplicates from the list of journals. \n",
    "\n",
    "### Resources\n",
    "- **Regular expressions:** a) https://docs.python.org/3.3/library/re.html, b) https://regexone.com, and c) https://docs.python.org/3/howto/regex.html. \n",
    "- ** HTML:** if you are not familiar with HTML see https://www.w3schools.com/html/ or one of the many tutorials on the internet. \n",
    "- ** Document Object Model (DOM):** for more on this programming interface for HTML and XML documents see https://www.w3schools.com/js/js_htmldom.asp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import the regular expressions library\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# use this file \n",
    "pub_filename = 'data/publist_super_clean.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# definition of get_pubs\n",
    "def get_pubs(filename: str) -> str:\n",
    "    #Open the file using the filename.\n",
    "    with open(filename, \"r\") as f:\n",
    "        prof_pubs=f.read()\n",
    "        #Return A string containing the HTML page ready to be parsed.\n",
    "        return (prof_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/publist_super_clean.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7f426fbc58ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprof_pubs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_pubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpub_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2b988c0dfe10>\u001b[0m in \u001b[0;36mget_pubs\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_pubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#Open the file using the filename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprof_pubs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#Return A string containing the HTML page ready to be parsed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/publist_super_clean.html'"
     ]
    }
   ],
   "source": [
    "prof_pubs=get_pubs(pub_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# check your code \n",
    "print (prof_pubs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You should see an HTML page\n",
    "```html\n",
    "<LI>\n",
    "<A HREF=\"Papers/2011/PhysRevB_84_125411_2011.pdf\" target=\"paper244\">\n",
    "&quot;Approaching the intrinsic band gap in suspended high-mobility graphene nanoribbons&quot;</A>\n",
    "<BR>Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng,\n",
    "<I>PHYSICAL REVIEW B </I> <b>84</b>,  125411 (2011)\n",
    "<BR>\n",
    "</LI>\n",
    "</OL>\n",
    "\n",
    "<OL START=243>\n",
    "<LI>\n",
    "<A HREF=\"Papers/2011/PhysRevB_84_035325_2011.pdf\" target=\"paper243\">\n",
    "&quot;Effect of symmetry breaking on the optical absorption of semiconductor nanoparticles&quot;</A>\n",
    "<BR>JAdam Gali, Efthimios Kaxiras, Gergely T. Zimanyi, Sheng Meng,\n",
    "<I>PHYSICAL REVIEW B </I> <b>84</b>,  035325 (2011)\n",
    "<BR>\n",
    "</LI>\n",
    "</OL>\n",
    "\n",
    "<OL START=242>\n",
    "<LI>\n",
    "<A HREF=\"Papers/2011/PhysRevB_83_054204_2011.pdf\" target=\"paper242\">\n",
    "&quot;Influence of CH2 content and network defects on the elastic properties of organosilicate glasses&quot;</A>\n",
    "<BR>Jan M. Knaup, Han Li, Joost J. Vlassak, and Efthimios Kaxiras,\n",
    "<I>PHYSICAL REVIEW B </I> <b>83</b>,  054204 (2011)\n",
    "<BR>\n",
    "</LI>\n",
    "</OL>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "prof_pubs.count('C.M. Friend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "search_input=r'<I>(.*)</I>'\n",
    "journals_raw=re.findall(search_input, prof_pubs, flags=0)\n",
    "journal=sorted(list(set(journals_raw)))\n",
    "journals=journal[0:2]+journal[3:] #Slice the list to get rid of the unreasonable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# check your code: print journals\n",
    "journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Your output should look like this (remember, no duplicates):\n",
    "```\n",
    "'ACSNano.',\n",
    " 'Ab initio',\n",
    " 'Ab-initio',\n",
    " 'Acta Mater.',\n",
    " 'Acta Materialia',\n",
    " 'Appl. Phys. Lett.',\n",
    " 'Applied Surface Science',\n",
    " 'Biophysical J.',\n",
    " 'Biosensing Using Nanomaterials',\n",
    "\n",
    " ...\n",
    "\n",
    " 'Solid State Physics',\n",
    " 'Superlattices and Microstructures',\n",
    " 'Surf. Sci.',\n",
    " 'Surf. Sci. Lett.',\n",
    " 'Surface  Science',\n",
    " 'Surface Review and Letters',\n",
    " 'Surface Sci. Lett.',\n",
    " 'Surface Science Lett.',\n",
    " 'Thin Solid Films',\n",
    " 'Top. Catal.',\n",
    " 'Z'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# our code here\n",
    "search_input2=r'<BR>(.*)[^\\n]\\n'\n",
    "pub_authors=re.findall(search_input2, prof_pubs, flags=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# check your code: print the list of strings containing the author(s)' names\n",
    "for item in pub_authors:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Your output should look like this (a line for each paper's author(s) string, with or without the comma)<BR><br>\n",
    "S. Meng and E. Kaxiras,<br>\n",
    "G. Lu and E. Kaxiras,<br>\n",
    "E. Kaxiras and S. Yip,<br>\n",
    "...<BR>\n",
    "Simone Melchionna, Efthimios Kaxiras, Massimo Bernaschi and Sauro Succi,<BR>\n",
    "J R Maze, A Gali, E Togan, Y Chu, A Trifonov,<BR>\n",
    "E Kaxiras, and M D Lukin,<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr style=\"height:1px\">\n",
    "\n",
    "### Question 2: Parsing and Converting to bibTex using Beautiful Soup\n",
    "\n",
    "A lot of the bibliographic and publication information is displayed in various websites in a not-so-structured HTML files. Some publishers prefer to store and transmit this information in a .bibTex file which has the following format:\n",
    "```\n",
    "@article { _number_\n",
    "     author = John Doyle\n",
    "     title = Interaction between atoms\n",
    "     URL = Papers/PhysRevB_81_085406_2010.pdf\n",
    "     journal = Phys. Rev. B\n",
    "     volume = 81\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "@article\n",
    "{    author = Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng\n",
    "     title = \"Approaching the intrinsic band gap in suspended high-mobility graphene nanoribbons\"\n",
    "     URL = Papers/2011/PhysRevB_84_125411_2011.pdf\n",
    "     journal = PHYSICAL REVIEW B\n",
    "     volume = 84\n",
    "}\n",
    "```\n",
    "About the [bibTex format](http://www.bibtex.org).\n",
    "\n",
    "In Question 2 you are given an .html file containing a list of papers scraped from the author's website and you are to write the information into .bibTex format. We used regular expressions for parsing HTML in the previous question but just regular expressions are hard to use in parsing real-life websites. A useful tool is [BeautifulSoup]  (http://www.crummy.com/software/BeautifulSoup/) (BS). You will parse the same file, this time using BS, which makes parsing HTML a lot easier.\n",
    "\n",
    "**2.1** Write a function called `make_soup` that accepts a filename for an HTML file and returns a BS object.\n",
    "    \n",
    "**2.2** Write a function that reads in the BS object, parses it, converts it into the .bibTex format using python string manipulation and regular expressions, and writes the data into `publist.bib`. You will need to create that file in your folder.  \n",
    "\n",
    "    \n",
    "#### HINT\n",
    "- Inspect the HTML code for tags that indicate information chunks such as `title` of the paper. You had already done this in Part 1 when you figured out how to get the name of the journal from the HTML code. The `find_all` method of BeautifulSoup might be useful.\n",
    "- Question 2.2 is better handled if you break the code into functions, each performing a small task such as finding the author(s) for each paper.\n",
    "- Make sure you catch exceptions when needed.\n",
    "- Regular expressions are a great tool for string manipulation. \n",
    "\n",
    "\n",
    "#### Resources\n",
    "- [BeautifulSoup Tutorial](https://www.dataquest.io/blog/web-scraping-tutorial-python/).\n",
    "- More about the [BibTex format](http://www.bibtex.org).<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries \n",
    "from bs4 import BeautifulSoup\n",
    "from sys import argv\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# definition of make_soup\n",
    "def make_soup(filename: str) -> BeautifulSoup:  \n",
    "    with open(filename, 'r') as f: #Open the html file\n",
    "        soup_html = BeautifulSoup(f, 'html.parser') #Parse html into BS format\n",
    "        #Return A BS object containing the HTML page\n",
    "        return (soup_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"data/publist_super_clean.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=make_soup(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check your code: print the Beautiful Soup object, you should see an HTML page\n",
    "print (soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"\n",
    "   \"http://www.w3.org/TR/html4/loose.dtd\">\n",
    "\n",
    "<title>Kaxiras E journal publications</title>\n",
    "<head>\n",
    "<meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
    "<link href=\"../styles/style_pubs.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
    "<meta content=\"\" name=\"description\"/>\n",
    "<meta content=\"Kaxiras E, Multiscale Methods, Computational Materials\" name=\"keywords\"/>\n",
    "</head>\n",
    "<body>\n",
    "<ol start=\"244\">\n",
    "<li>\n",
    "<a href=\"Papers/2011/PhysRevB_84_125411_2011.pdf\" target=\"paper244\">\n",
    "\"Approaching the intrinsic band gap in suspended high-mobility graphene nanoribbons\"</a>\n",
    "<br/>Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng,\n",
    "<i>PHYSICAL REVIEW B </i> <b>84</b>,  125411 (2011)\n",
    "<br/>\n",
    "</li>\n",
    "</ol>\n",
    "<ol start=\"243\">\n",
    "<li>\n",
    "<a href=\"Papers/2011/PhysRevB_84_035325_2011.pdf\" target=\"paper243\">\n",
    "\"Effect of symmetry breaking on the optical absorption of semiconductor nanoparticles\"</a>\n",
    "<br/>JAdam Gali, Efthimios Kaxiras, Gergely T. Zimanyi, Sheng Meng,\n",
    "<i>PHYSICAL REVIEW B </i> <b>84</b>,  035325 (2011)\n",
    "<br/>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_to_bib(filename):\n",
    "    results=filename.find_all('ol') #Find 'ol' tag, and save the contents in results\n",
    "    with open('data/publist.bib', 'w') as bibfile: #Create a writable file named publist.bib\n",
    "        for result in results:\n",
    "            bibfile.write('@article {'+'\\n')\n",
    "            #Locate the author names and add into the bibfile\n",
    "            bibfile.write('author = {0}'.format(result.contents[1].contents[4][:-3])+'\\n') \n",
    "            #Locate titles and add into the bibfile\n",
    "            bibfile.write('title = {0}'.format(result.contents[1].contents[1].text[2:-1])+'\\n') \n",
    "            #Locate the links and add into the bibfile\n",
    "            bibfile.write('URL = {0}'.format(result.contents[1].contents[1]['href'])+'\\n')\n",
    "            #Locate the journal names and add into the bibfile\n",
    "            bibfile.write('journal = {0}'.format(result.contents[1].contents[5].text)+'\\n')\n",
    "            #Locate the volume numbers and add into the bibfile\n",
    "            bibfile.write('volume = {0}'.format(result.contents[1].contents[7].text)+'\\n') \n",
    "            bibfile.write('}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_to_bib(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publist=open('data/publist.bib', 'r')\n",
    "print(publist.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Your output should look like this\n",
    "```\n",
    "@article\n",
    "{    author = Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng\n",
    "     title = \"Approaching the intrinsic band gap in suspended high-mobility graphene nanoribbons\"\n",
    "     URL = Papers/2011/PhysRevB_84_125411_2011.pdf\n",
    "     journal = PHYSICAL REVIEW B\n",
    "     volume = 84\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "@article\n",
    "{    author = E. Kaxiras and S. Succi\n",
    "     title = \"Multiscale simulations of complex systems: computation meets reality\"\n",
    "     URL = Papers/SciModSim_15_59_2008.pdf\n",
    "     journal = Sci. Model. Simul.\n",
    "     volume = 15\n",
    "}\n",
    "@article\n",
    "{    author = E. Manousakis, J. Ren, S. Meng and E. Kaxiras\n",
    "     title = \"Effective Hamiltonian for FeAs-based superconductors\"\n",
    "     URL = Papers/PhysRevB_78_205112_2008.pdf\n",
    "     journal = Phys. Rev. B\n",
    "     volume = 78\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part B [50 pts]:  Follow the stars in IMDb's list of \"The Top 100 Stars for 2017\" \n",
    "\n",
    "### Overview\n",
    "\n",
    "In Part 3 your goal is to extract information from IMDb's Top 100 Stars for 2017 (https://www.imdb.com/list/ls025814950/) and perform some analysis on each star in the list. In particular we are interested to know: a) how many performers made their first movie at 17? b) how many performers started as child actors? c) who is the most proliferate actress or actor in IMDb's list of the Top 100 Stars for 2017? . These questions are addressed in more details in the Questions below. \n",
    "\n",
    "When data is **not** given to us in a file, we need to fetch them using one of the following ways:\n",
    "- download a file from a source URL\n",
    "- query a database \n",
    "- query a web API \n",
    "- scrape data from the web page\n",
    "\n",
    "### Question 1: Web Scraping Using Beautiful Soup\n",
    "**1.1** Download the webpage of the \"Top 100 Stars for 2017\" (https://www.imdb.com/list/ls025814950/) into a `requests` object and name it `my_page`. Explain what the following attributes are:\n",
    "\n",
    "- `my_page.text`, \n",
    "- `my_page.status_code`,\n",
    "- `my_page.content`.\n",
    "\n",
    "**1.2** Create a Beautiful Soup object named `star_soup` giving `my_page` as input.\n",
    "\n",
    "**1.3** Write a function called `parse_stars` that accepts `star_soup` as its input and generates a list of dictionaries named `starlist` (see definition below). One of the fields of this dictionary is the `url` of each star's individual page, which you need to scrape and save the contents in the `page` field. Note that there is a ton of information about each star on these webpages. \n",
    "\n",
    "**1.4** Write a function called `create_star_table` to extract information about each star (see function definition for the exact information to extract). **Only extract information from the first box on each star's page. If the first box is acting, consider only acting credits and the star's acting debut, if the first box is Directing, consider only directing credits and directorial debut.**\n",
    "\n",
    "**1.5** Now that you have scraped all the info you need, it's a good practice to save the last data structure you created to disk. That way if you need to re-run from here, you don't need to redo all these requests and parsing. Save this information to a JSON file and **submit** this JSON file in Canvas with your notebook. \n",
    "\n",
    "**1.6** Import the contents of the teaching staff's JSON file (`data/staff_starinfo.json`) into a pandas dataframe. Check the types of variables in each column and clean these variables if needed. Add a new column to your dataframe with the age of each actor when they made first movie (name this column `age_at_first_movie`).\n",
    "\n",
    "**1.7** You are now ready to answer the following intriguing questions: \n",
    "- How many performers made their first movie at 17?\n",
    "- How many performers started as child actors? Define child actor as a person less than 12 years old. \n",
    "- Who is the most prolific actress or actor in IMDb's list of the Top 100 Stars for 2017? \n",
    "\n",
    "**1.8** Make a plot of the number of credits versus the name of actor/actress. \n",
    " \n",
    "\n",
    "\n",
    "### Hints\n",
    "- Create a variable that groups actors/actresses by the age of their first movie. Use pandas' `.groupby` to divide the dataframe into groups of performers that for example started performing as children (age $<$ 12). The grouped variable is a `GroupBy` pandas object and this object has all of the information needed to then apply some operation to each of the groups.\n",
    "- When cleaning the data make sure the variables with which you are performing calculations are in numerical format.\n",
    "- The column with the year has some values that are double, e.g. **'2000-2001'** and the column with age has some empty cells. You need to deal with these before performing calculations on the data! \n",
    "- You should include both movies and TV shows.\n",
    "    \n",
    "### Resources\n",
    "- The `requests` library makes working with HTTP requests powerful and easy. For more on the `requests` library see http://docs.python-requests.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Get the IMDB page\n",
    "my_page = requests.get(\"https://www.imdb.com/list/ls025814950/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<HR> Your answers here\n",
    "\n",
    "<HR>my_page.text: The text associated with this webpage\n",
    "\n",
    "my_page.status.code: HTTP status number, tells if the server successfully answered the request\n",
    "\n",
    "my_page.content: HTML content of the page, including the children nested in the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "star_soup = BeautifulSoup(my_page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# check your code - you should see an HTML page\n",
    "print (star_soup.prettify()[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "```\n",
    "Function\n",
    "--------\n",
    "parse_stars\n",
    "\n",
    "Input\n",
    "------\n",
    "star_soup: the soup object with the scraped page\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "a list of dictionaries; each dictionary corresponds to a star profile and has the following data:\n",
    "\n",
    "    name: the name of the actor/actress as it appears at the top\n",
    "    gender: 0 or 1: translate the word 'actress' into 1 and 'actor' into '0'\n",
    "    url: the url of the link under their name that leads to a page with details\n",
    "    page: the string containing the soup of the text in their individual info page (from url)\n",
    "\n",
    "Example:\n",
    "--------\n",
    "{'name': Tom Hardy,\n",
    "  'gender': 0,\n",
    "  'url': https://www.imdb.com/name/nm0362766/?ref_=nmls_hd,\n",
    "  'page': BS object with 'html text acquired by scraping the 'url' page'\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gender(filename): #Function to analysis the description of the star and determine the gender\n",
    "    word_list = []\n",
    "    file1=filename.split()\n",
    "    for word in file1:\n",
    "        word_list.append(word.split(',')[0])\n",
    "    gender=0\n",
    "    for word in word_list:\n",
    "        if word=='actress':\n",
    "            gender=1\n",
    "    return (gender)\n",
    "\n",
    "def parse_stars(soupfile):\n",
    "    find_div=soupfile.find_all('div', attrs={'class':'lister-item mode-detail'}) #Find all informations\n",
    "    star_list=[]\n",
    "    for div in find_div:\n",
    "        star_profile=collections.OrderedDict()\n",
    "        star_profile['name']=div.contents[3].contents[1].contents[3].text[1:-1] #Locate the name and extract the text\n",
    "        #Locate the bio of the actor, and call the gender function to determine the gender\n",
    "        star_profile['gender']=find_gender(div.contents[3].contents[5].text)\n",
    "        #Make a full linke of the star\n",
    "        star_profile['URL']='https://www.imdb.com'+div.contents[3].contents[1].contents[3]['href']\n",
    "        #Request the personal page of the star, and parse into soup\n",
    "        star_profile['page']=BeautifulSoup((requests.get('https://www.imdb.com'+div.contents[3].contents[1].contents[3]['href'])).text, 'html.parser')\n",
    "        star_list.append(star_profile) #Add the dictionaries into the list\n",
    "        time.sleep(10)\n",
    "    return(star_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starlist=parse_stars(star_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# this list is large because of the html code into the `page` field\n",
    "# to get a better picture, print only the first element\n",
    "starlist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "{'name': 'Gal Gadot',\n",
    " 'gender': 1,\n",
    " 'url': 'https://www.imdb.com/name/nm2933757?ref_=nmls_hd',\n",
    " 'page': \n",
    " <!DOCTYPE html>\n",
    " \n",
    " <html xmlns:fb=\"http://www.facebook.com/2008/fbml\" xmlns:og=\"http://ogp.me/ns#\">\n",
    " <head>\n",
    " <meta charset=\"utf-8\"/>\n",
    " <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
    " <meta content=\"app-id=342792525, app-argument=imdb:///name/nm2933757?src=mdot\" name=\"apple-itunes-app\"/>\n",
    " <script type=\"text/javascript\">var IMDbTimer={starttime: new Date().getTime(),pt:'java'};</script>\n",
    " <script>\n",
    "     if (typeof uet == 'function') {\n",
    "       uet(\"bb\", \"LoadTitle\", {wb: 1});\n",
    "     }\n",
    " </script>\n",
    " <script>(function(t){ (t.events = t.events || {})[\"csm_head_pre_title\"] = new Date().getTime(); })(IMDbTimer);</script>\n",
    " \n",
    "... \n",
    "\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "```\n",
    "Function\n",
    "--------\n",
    "create_star_table\n",
    "\n",
    "Input\n",
    "------\n",
    "the starlist\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a list of dictionaries; each dictionary corresponds to a star profile and has the following data:\n",
    "\n",
    "    star_name: the name of the actor/actress as it appears at the top\n",
    "    gender: 0 or 1 (1 for 'actress' and 0 for 'actor')  \n",
    "    year_born : year they were born\n",
    "    first_movie: title of their first movie or TV show\n",
    "    year_first_movie: the year they made their first movie or TV show\n",
    "    credits: number of movies or TV shows they have made in their career.\n",
    "    \n",
    "--------\n",
    "Example:\n",
    "\n",
    "{'star_name': Tom Hardy,\n",
    "  'gender': 0,\n",
    "  'year_born': 1997,\n",
    "  'first_movie' : 'Batman',\n",
    "  'year_first_movie' : 2017,\n",
    "  'credits' : 24}\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#To find the role of the stars\n",
    "def find_actor(filename):\n",
    "    actor=filename['page'].find_all('div', attrs={'class':'head'})[0].contents[5].text\n",
    "    return (actor)\n",
    "#To find the sequence number of the first movie\n",
    "def find_movie(filename1, filename2):\n",
    "    if (find_actor(filename1)=='Actor' or find_actor(filename1)=='Actress'):\n",
    "        s=r'act_(.*)' #If the star's role is an actor or actress\n",
    "    else:\n",
    "        s=r'wr_(.*)' #If the star's role is a writer\n",
    "    return(re.findall(s, filename2))\n",
    "#Function to find the name and year of the first movie\n",
    "def find_first_movie(filename):\n",
    "    credits=get_credits(filename)\n",
    "    if credits%2==0: #Find the div with the information of first movie\n",
    "        search=filename['page'].find_all('div', attrs={'class':'filmo-row even'})\n",
    "    else:\n",
    "        search=filename['page'].find_all('div', attrs={'class':'filmo-row odd'})\n",
    "    movie=''\n",
    "    year=''\n",
    "    for div in search:\n",
    "        num=find_movie(filename, div.contents[3].contents[0]['href']) #Find the credits of first box\n",
    "        if int(num[0])==credits:\n",
    "            movie=div.contents[3].text #Extract the movie name\n",
    "            year=div.contents[1].text[2:6] #Extract the movie year\n",
    "            return (movie, year)\n",
    "            break\n",
    "def get_credits(filename): #Function to find the credits of the first box\n",
    "    return (int(filename['page'].find_all('div', attrs={'class':'head'})[0].contents[6][2:-10]))\n",
    "def get_year_born(filename): #Function of find the year_born of the star\n",
    "    year_born=filename['page'].find_all('script', attrs={'type':'application/ld+json'})[0].contents[0][-13:-9]\n",
    "    if (year_born.isnumeric()):\n",
    "        return (year_born)\n",
    "    else:\n",
    "        return ('NA')\n",
    "def create_star_table(filename:list)-> list:\n",
    "    new_starlist=[]\n",
    "    for star in filename:\n",
    "        star_dic={}\n",
    "        star_dic[\"star_name\"]=star['name'] #Add star's name\n",
    "        star_dic['gender']=star['gender'] #Add star's gender\n",
    "        star_dic['year_born']= get_year_born(star) #Add star's birth year\n",
    "        star_dic['first_movie']=find_first_movie(star)[0] #Add the first movie the star performed\n",
    "        star_dic['year_first_movie']=int(find_first_movie(star)[1]) #Add the year of the first movie\n",
    "        star_dic['credits']=get_credits(star) #Add the credits of the star\n",
    "        new_starlist.append(star_dic)\n",
    "    return(new_starlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL ONLY ONCE - IT WILL TAKE SOME TIME TO RUN\n",
    "star_table = []\n",
    "star_table = create_star_table(starlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# check your code\n",
    "star_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "[{'name': 'Gal Gadot',\n",
    "  'gender': 1,\n",
    "  'year_born': '1985',\n",
    "  'first_movie': 'Bubot',\n",
    "  'year_first_movie': '2007',\n",
    "  'credits': '25'},\n",
    " {'name': 'Tom Hardy',\n",
    "  'gender': 0,\n",
    "  'year_born': '1977',\n",
    "  'first_movie': 'Tommaso',\n",
    "  'year_first_movie': '2001',\n",
    "  'credits': '55'},\n",
    "  \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/star_table.json', 'w') as output:   #Write the list of dictionary into a json file.\n",
    "    json.dump(star_table, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load starinfo data\n",
    "with open('data/staff_starinfo.json', 'r') as f:\n",
    "    stars_data = json.load(f)\n",
    "\n",
    "stars_data_json_str = json.dumps(stars_data) # Convert data to json string\n",
    "df = pd.read_json(stars_data_json_str) # Convert to pandas dataframe\n",
    "df.head() # Look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_at_first_movie=[]\n",
    "for i in range(0,len(df)):\n",
    "    #Obtain the age_at_first_movie by comparing year_born and year_first_movie.\n",
    "    num=int(df['year_first_movie'][i][0:4])-df['year_born'][i] \n",
    "    age_at_first_movie.append(num) #Put all the numbers into a list.\n",
    "df['age_at_first_movie']=age_at_first_movie #Add a new column to df.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "num=len(df[df['age_at_first_movie']==17])#Slice dataframe with age_at_first movie equals to 17.\n",
    "print(\"{0} performers made their first movie at 17.\".format(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:<BR>\n",
    "8 performers made their first movie at 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "num1=len(df[df['age_at_first_movie']<12]) #Slice dataframe with age_at_first movie less than 12.\n",
    "print(\"{0} performers started as child actors.\".format(num1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "df.sort_values(by='credits', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the most prolific actor in 2017 is Sean Young."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "for i in [0, 26, 51, 76]:\n",
    "    plt.plot(df['name'][i:i+25], df['credits'][i:i+25], 'o', color='red')\n",
    "    plt.title(\"Credits of the Stars\")\n",
    "    plt.xlabel(\"Name of the Stars\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Credits of the Stars\")\n",
    "    plt.ylim(1, 150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<HR>\n",
    "\n",
    "Your answer here\n",
    "\n",
    "<HR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling(): styles = open(\"styles/cs109.css\", \"r\").read(); return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
